# @Time : 2022-04-04 8:45
# @Author : Phalange
# @File : tokenizer.py
# @Software: PyCharm
# C'est la vie,enjoy it! :D

from tokenizers import Tokenizer
from tokenizers.models import BPE
from tokenizers.trainers import BpeTrainer
from tokenizers.pre_tokenizers import Whitespace
from tokenizers.processors import TemplateProcessing
from tokenizers import BertWordPieceTokenizer

from time import time
tokenizer = Tokenizer(BPE(unk_token="[UNK]"))
# vocab_sizeé»˜è®¤ä¸º30000ï¼Œmin_freqé»˜è®¤ä¸º0
# [UNK]çš„ä½ç½®å°†åœ¨0ï¼ŒCLSçš„ä½ç½®åœ¨1
trainer = BpeTrainer(special_tokens=["[UNK]","[CLS]","[SEP]","[PAD]","[MASK]"],vocab_size=30000,min_frequency=5)

# ä½¿ç”¨é¢„åˆ†è¯å™¨å°†ç¡®ä¿æ²¡æœ‰ä»»ä½•ä»¤ç‰Œå¤§äºé¢„åˆ†è¯å™¨è¿”å›çš„å•è¯ã€‚æ¯”å¦‚it is å¸¸ç»„åˆåœ¨ä¸€æ¬¡ï¼Œä½†ä»ç„¶éœ€è¦æŠŠä»–ä»¬åˆ†è¯åˆ†å¼€
tokenizer.pre_tokenizer = Whitespace()

# train
files = [f"../dataset/wikitext-103-raw/wiki.{split}.raw" for split in ["train","test","valid"]]
start_time = time()
#tokenizer.train(files,trainer)
end_time = time()
print("spend time:" + str(float(end_time - start_time)))
#tokenizer.save("../dataset/tokenizer-wiki.json")
print("save success!")

#---------------------------------------------------------------------------------------------

# reload
tokenizer2 = Tokenizer.from_file("../dataset/tokenizer-wiki.json")
print("reload success!")
# using the tokenizer
sentence = "Hello,y'all! How are youğŸ¤—?"
output = tokenizer2.encode(sentence)
print(output.tokens)
print(output.ids) # æ‰“å°è¯åœ¨è¯å…¸ä¸­çš„ä½ç½®
#---------------------------------------------------------------------------------------------

"""
ä»ä¸Šé¢çš„ç»“æœå¯ä»¥çœ‹å‡ºæ¥ï¼Œå€’æ•°ç¬¬äºŒä½ç½®æ˜¯ä¸€ä¸ª'UNK',å› ä¸ºè¯å…ƒåº“æœ‰å…¨å¯¹é½è·Ÿè¸ªçš„åŠŸèƒ½ï¼Œæ‰€ä»¥å¯ä»¥çŸ¥é“è¿™ä¸ªUNK
çš„åŸå§‹è¯æ˜¯ä»€ä¹ˆï¼Œä½¿ç”¨offsetså¯ä»¥æŸ¥çœ‹å‡ºä»–åœ¨æœ¬æ¥å¥å­ä¸­çš„åç§»
"""
# æ ¹æ®output.toeknsç»“æœçœ‹å‡º,0çš„è¾“å‡ºä½ç½®æ˜¯-2
emoj_site = output.offsets[-2]
print(emoj_site)
print(sentence[emoj_site[0]:emoj_site[1]])

# é€šè¿‡BpeTraineræˆ‘ä»¬è®¾ç½®äº†SEPçš„ä½ç½®æ˜¯2ï¼Œå…¶å®å¯ä»¥é€šè¿‡ä¸‹é¢çš„æ–¹æ³•å»ç´¢å¼•ï¼Œæ¥å†éªŒè¯ï¼š
print(tokenizer2.token_to_id("[SEP]"))



#---------------------------------------------------------------------------------------------

# post-processingï¼Œfor the traditional BERT inputs

tokenizer2.post_processor = TemplateProcessing(
    single="[CLS] $A [SEP]", # $Aä»£è¡¨çš„æ˜¯å¥å­
    pair="[CLS] $A [SEP] $B:1 [SEP]:1", #å…·ä½“äº†å¯¹äºå¥å­å¯¹çš„æ¨¡æ¿,$Aæ˜¯ç¬¬ä¸€ä¸ªå¥å­ï¼Œ$Bæ˜¯ç¬¬äºŒä¸ªå¥å­,åœ¨æ¨¡æ¿ä¸­æ·»åŠ çš„è¡¨ç¤ºæˆ‘ä»¬å¸Œæœ›è¾“å…¥çš„æ¯ä¸ªéƒ¨åˆ†çš„ç±»å‹ ID:1
                                        #å› ä¸ºé»˜è®¤ä¸º0ï¼Œæ‰€ä»¥æˆ‘ä»¬ä¸ç”¨$A:0
                                        # æ‰€ä»¥è¿™é‡Œå°±æ˜¯å§æŠŠç¬¬äºŒä¸ªå¥å­ï¼Œå’Œç¬¬äºŒä¸ª[SEP]çš„IDè®¾ç½®ä¸º1
    # æœ€åï¼Œæˆ‘ä»¬åœ¨æ ‡è®°å™¨çš„è¯æ±‡è¡¨ä¸­æŒ‡å®šæˆ‘ä»¬ä½¿ç”¨çš„ç‰¹æ®Šæ ‡è®°åŠå…¶ IDã€‚
    special_tokens=[
        ("[CLS]",tokenizer2.token_to_id("[CLS]")),
        ("[SEP]",tokenizer2.token_to_id("[SEP]")),
    ]
)

# æ£€æµ‹æ˜¯å¦ä¸Šä¸€æ­¥å¤„ç†æ­£ç¡®
# æ£€æµ‹å¥å­
output = tokenizer2.encode(sentence)
print(output.tokens)
# æ£€æµ‹å¥å­å¯¹
output = tokenizer2.encode("Hello,y'all","How are youğŸ˜?")
print(output.tokens)
# æ£€æŸ¥ä¸€ä¸‹IDSçš„åˆ†å¸ƒæ˜¯å¦æ­£ç¡®ï¼š
print(output.type_ids)
# å¦‚æœæ­¤æ—¶saveï¼Œåˆ™post-processorä¹Ÿä¼šè¢«save
#tokenizer2.save("../dataset/tokenizer-wiki.json")

#---------------------------------------------------------------------------------------------

# ä¸ºäº†åŠ é€Ÿå¤„ç†textï¼Œå¯ä»¥é€šè¿‡ä½¿ç”¨æŒ‰æ‰¹ç¼–ç çš„æ–¹æ³•å»ç¼–ç å¤šä¸ªå¥å­
# è‡ªåŠ¨å¡«å……åˆ°æœ€é•¿å¥å­ï¼Œä¹Ÿå¯ä»¥æŒ‡å®šä¸€ä¸ªé•¿åº¦,è¿™ä¸ªæ–¹æ³•çš„é»˜è®¤å¡«å……æ–¹å‘æ˜¯å‘å³
tokenizer2.enable_padding(pad_id=tokenizer2.token_to_id("[UNK]"),pad_token="[UNK]")


output = tokenizer.encode_batch(["Hello, y'all!", "How are you ğŸ˜ ?"])
print(output[1].tokens)

# In this case, the attention mask generated by the tokenizer takes the padding into account
print(output[1].attention_mask) # æäº¤æ³¨æ„åŠ›æ¨¡


#-----------------------------------------------------------------------------------------------------------------------------------------
# ä½¿ç”¨é¢„è®­ç»ƒçš„è¯å…ƒ
tokenizer3 = Tokenizer.from_pretrained("bert-base-uncased")

# é€šè¿‡ç›´æ¥å¯¼å…¥ä¸€ä¸ªé¢„è®­ç»ƒçš„åˆ†è¯å™¨ï¼Œåªè¦æœ‰ä»–çš„è¯æ±‡æ–‡ä»¶
tokenizer = BertWordPieceTokenizer("bert-base-uncased-vocab.txt", lowercase=True)
