# @Time : 2022-04-04 8:45
# @Author : Phalange
# @File : tokenizer.py
# @Software: PyCharm
# C'est la vie,enjoy it! :D

from tokenizers import Tokenizer
from tokenizers.models import BPE
from tokenizers.trainers import BpeTrainer
from tokenizers.pre_tokenizers import Whitespace
from tokenizers.processors import TemplateProcessing
from tokenizers import BertWordPieceTokenizer

from time import time
tokenizer = Tokenizer(BPE(unk_token="[UNK]"))
# vocab_size默认为30000，min_freq默认为0
# [UNK]的位置将在0，CLS的位置在1
trainer = BpeTrainer(special_tokens=["[UNK]","[CLS]","[SEP]","[PAD]","[MASK]"],vocab_size=30000,min_frequency=5)

# 使用预分词器将确保没有任何令牌大于预分词器返回的单词。比如it is 常组合在一次，但仍然需要把他们分词分开
tokenizer.pre_tokenizer = Whitespace()

# train
files = [f"../dataset/wikitext-103-raw/wiki.{split}.raw" for split in ["train","test","valid"]]
start_time = time()
#tokenizer.train(files,trainer)
end_time = time()
print("spend time:" + str(float(end_time - start_time)))
#tokenizer.save("../dataset/tokenizer-wiki.json")
print("save success!")

#---------------------------------------------------------------------------------------------

# reload
tokenizer2 = Tokenizer.from_file("../dataset/tokenizer-wiki.json")
print("reload success!")
# using the tokenizer
sentence = "Hello,y'all! How are you🤗?"
output = tokenizer2.encode(sentence)
print(output.tokens)
print(output.ids) # 打印词在词典中的位置
#---------------------------------------------------------------------------------------------

"""
从上面的结果可以看出来，倒数第二位置是一个'UNK',因为词元库有全对齐跟踪的功能，所以可以知道这个UNK
的原始词是什么，使用offsets可以查看出他在本来句子中的偏移
"""
# 根据output.toekns结果看出,0的输出位置是-2
emoj_site = output.offsets[-2]
print(emoj_site)
print(sentence[emoj_site[0]:emoj_site[1]])

# 通过BpeTrainer我们设置了SEP的位置是2，其实可以通过下面的方法去索引，来再验证：
print(tokenizer2.token_to_id("[SEP]"))



#---------------------------------------------------------------------------------------------

# post-processing，for the traditional BERT inputs

tokenizer2.post_processor = TemplateProcessing(
    single="[CLS] $A [SEP]", # $A代表的是句子
    pair="[CLS] $A [SEP] $B:1 [SEP]:1", #具体了对于句子对的模板,$A是第一个句子，$B是第二个句子,在模板中添加的表示我们希望输入的每个部分的类型 ID:1
                                        #因为默认为0，所以我们不用$A:0
                                        # 所以这里就是吧把第二个句子，和第二个[SEP]的ID设置为1
    # 最后，我们在标记器的词汇表中指定我们使用的特殊标记及其 ID。
    special_tokens=[
        ("[CLS]",tokenizer2.token_to_id("[CLS]")),
        ("[SEP]",tokenizer2.token_to_id("[SEP]")),
    ]
)

# 检测是否上一步处理正确
# 检测句子
output = tokenizer2.encode(sentence)
print(output.tokens)
# 检测句子对
output = tokenizer2.encode("Hello,y'all","How are you😁?")
print(output.tokens)
# 检查一下IDS的分布是否正确：
print(output.type_ids)
# 如果此时save，则post-processor也会被save
#tokenizer2.save("../dataset/tokenizer-wiki.json")

#---------------------------------------------------------------------------------------------

# 为了加速处理text，可以通过使用按批编码的方法去编码多个句子
# 自动填充到最长句子，也可以指定一个长度,这个方法的默认填充方向是向右
tokenizer2.enable_padding(pad_id=tokenizer2.token_to_id("[UNK]"),pad_token="[UNK]")


output = tokenizer.encode_batch(["Hello, y'all!", "How are you 😁 ?"])
print(output[1].tokens)

# In this case, the attention mask generated by the tokenizer takes the padding into account
print(output[1].attention_mask) # 提交注意力模


#-----------------------------------------------------------------------------------------------------------------------------------------
# 使用预训练的词元
tokenizer3 = Tokenizer.from_pretrained("bert-base-uncased")

# 通过直接导入一个预训练的分词器，只要有他的词汇文件
tokenizer = BertWordPieceTokenizer("bert-base-uncased-vocab.txt", lowercase=True)
